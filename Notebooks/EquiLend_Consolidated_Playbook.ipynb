{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "7019a"
   },
   "source": [
    "## üîß **Why Python Modules vs Notebooks?**\n",
    "\n",
    "### **The Hybrid Approach Philosophy**\n",
    "\n",
    "This workspace uses both **Python modules** (`.py` files) and **Jupyter notebooks** (`.ipynb` files) strategically:\n",
    "\n",
    "#### üìì **Use Notebooks When:**\n",
    "- **Interactive exploration** - Testing ideas, parameter tuning, data exploration\n",
    "- **Visualization & reporting** - Charts, analysis reports, stakeholder presentations\n",
    "- **One-off analysis** - Custom client requests, ad-hoc research questions\n",
    "- **Model development** - Prototyping new factors before productionizing\n",
    "- **Documentation** - Explaining methodology with embedded code and results\n",
    "\n",
    "#### üêç **Use Python Modules When:**\n",
    "- **Reusable business logic** - Factor calculations used across multiple analyses\n",
    "- **Production automation** - Scripts that run on schedule without human interaction\n",
    "- **Clean, testable code** - Functions that need unit testing and version control\n",
    "- **Performance-critical** - Code processing large datasets efficiently\n",
    "- **Importable functions** - Logic that needs to be imported into other scripts\n",
    "\n",
    "### **Our Organized Structure:**\n",
    "\n",
    "```\n",
    "Bob_EquiLend_Models/\n",
    "‚îú‚îÄ‚îÄ üè≠ PRODUCTION (Python Modules)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ models/core_factors.py        # Factor calculations\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ models/extended_factors.py    # Advanced models\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ src/daily_digest.py           # Report generation\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ üìä INTERACTIVE (Notebooks)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ EquiLend_Consolidated_Playbook.ipynb  # This notebook\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Factor_Development.ipynb              # New factor prototyping\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ üìÅ SUPPORTING\n",
    "    ‚îú‚îÄ‚îÄ data/           # Your securities lending data\n",
    "    ‚îî‚îÄ‚îÄ documentation/  # Word docs and specifications\n",
    "```\n",
    "\n",
    "### **Workflow in Practice:**\n",
    "\n",
    "1. **üî¨ Research** ‚Üí Develop new ideas in `Factor_Development.ipynb`\n",
    "2. **üèóÔ∏è Build** ‚Üí Move stable code to modules (`models/core_factors.py`)\n",
    "3. **üìä Analyze** ‚Üí Import modules into this playbook for daily analysis  \n",
    "4. **üöÄ Deploy** ‚Üí Use modules directly for automated production runs\n",
    "\n",
    "**Result:** You get notebook interactivity for research AND clean, maintainable modules for production!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bdd44d",
   "metadata": {
    "cellUniqueIdByVincent": "ac39f"
   },
   "outputs": [],
   "source": [
    "# Main Jupyter Notebook for EquiLend Financing Models\n",
    "# Consolidated playbook with all models and analysis\n",
    "\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# EquiLend Equity Financing Research Playbook  \\n\",\n",
    "    \"*Generated: 2025-06-29*  \\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook consolidates all models discussed ‚Äì short-squeeze analytics **and** broader equity-financing factors such as credit-equity basis, options-skew divergence, ETF flow pressure, macro-liquidity overlay, ESG constraints, crowd buzz and more.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Setup and Imports\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from datetime import date, datetime\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add organized models to path\\n\",\n",
    "    \"sys.path.append('../models')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import our organized models\\n\",\n",
    "    \"from core_factors import (\\n\",\n",
    "    \"    ShortInterestMomentum, BorrowCostShock, UtilizationPersistence,\\n\",\n",
    "    \"    FeeTrendZScore, DaysToCoverZ, LocateProxyFactor, compute_all_factors\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"from extended_factors import (\\n\",\n",
    "    \"    BorrowCDSBasis, OptionsSkewDivergence, ETFFlowPressure,\\n\",\n",
    "    \"    MacroLiquidityStress, ESGConstraintGauge, CrowdBuzzPulse,\\n\",\n",
    "    \"    EnhancedShortSqueezeV4, compute_extended_factors\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"EquiLend Financing Models Playbook - {date.today()}\\\")\\n\",\n",
    "    \"print(\\\"All models loaded successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Environment Setup\\n\",\n",
    "    \"\\n\",\n",
    "    \"Set up your API keys in a `.env` file:\\n\",\n",
    "    \"```bash\\n\",\n",
    "    \"FRED_KEY=your_fred_token\\n\",\n",
    "    \"POLYGON_KEY=your_polygon_token\\n\",\n",
    "    \"GEMINI_API_KEY=your_gemini_key\\\\n\\\",\n",
    "    \"SQL_URI=your_database_connection\\n\",\n",
    "    \"```\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load environment variables\\n\",\n",
    "    \"from dotenv import load_dotenv\\n\",\n",
    "    \"load_dotenv()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Verify key environment variables\\n\",\n",
    "    \"required_keys = ['FRED_KEY', 'GEMINI_API_KEY']\\\\n\\\",\n",
    "    \"missing_keys = [key for key in required_keys if not os.getenv(key)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"if missing_keys:\\n\",\n",
    "    \"    print(f\\\"‚ö†Ô∏è  Missing environment variables: {missing_keys}\\\")\\n\",\n",
    "    \"    print(\\\"Some features may not work without proper API keys\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"‚úÖ All required environment variables found\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Complete Model Catalog\\n\",\n",
    "    \"\\n\",\n",
    "    \"The table below lists every model we have covered so far.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Model catalog\\n\",\n",
    "    \"models_data = {\\n\",\n",
    "    \"    'Model': [\\n\",\n",
    "    \"        'Short-Interest Momentum (SIM)',\\n\",\n",
    "    \"        'Borrow Cost Shock (BCS)',\\n\",\n",
    "    \"        'Utilisation Persistence (UPI)',\\n\",\n",
    "    \"        'Fee Trend Z-Score (FTZ)',\\n\",\n",
    "    \"        'Locate Proxy Factor (LPF)',\\n\",\n",
    "    \"        'Days-To-Cover Z (DTC_z)',\\n\",\n",
    "    \"        'Borrow-CDS Basis',\\n\",\n",
    "    \"        'Options Skew Divergence',\\n\",\n",
    "    \"        'ETF Flow Pressure',\\n\",\n",
    "    \"        'Macro Liquidity Stress',\\n\",\n",
    "    \"        'ESG Constraint Gauge',\\n\",\n",
    "    \"        'Crowd Buzz Pulse',\\n\",\n",
    "    \"        'Enhanced Short Squeeze Prediction (SSR v4)'\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    'Inputs': [\\n\",\n",
    "    \"        'ŒîOn-Loan Qty, ŒîFee',\\n\",\n",
    "    \"        '1-day fee spike vs 30-day œÉ',\\n\",\n",
    "    \"        '20-day avg Util',\\n\",\n",
    "    \"        '20-day fee slope',\\n\",\n",
    "    \"        'Re-Rate Ratio, B2B loans',\\n\",\n",
    "    \"        'SI √∑ ADV',\\n\",\n",
    "    \"        'Fee All, ICE OAS (FRED)',\\n\",\n",
    "    \"        'Fee, Util, CBOE SKEW (FRED)',\\n\",\n",
    "    \"        'On-Loan Qty, ETF ŒîSharesOut',\\n\",\n",
    "    \"        'Util, STLFSI2 (FRED)',\\n\",\n",
    "    \"        'Lender Count, MSCI ESG',\\n\",\n",
    "    \"        'Reddit/X mention velocity',\\n\",\n",
    "    \"        'SIM+BCS+UPI+Buzz+Uptick'\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    'Purpose': [\\n\",\n",
    "    \"        'Gauge accelerating short build-up',\\n\",\n",
    "    \"        'Detect sudden scarcity events',\\n\",\n",
    "    \"        'Persistent tight supply',\\n\",\n",
    "    \"        'Under-the-radar fee drifts',\\n\",\n",
    "    \"        'Stand-in for locate surges',\\n\",\n",
    "    \"        'Short-covering pressure',\\n\",\n",
    "    \"        'Credit-equity dislocations',\\n\",\n",
    "    \"        'Hedge mis-pricing signal',\\n\",\n",
    "    \"        'Arbitrage strain detection',\\n\",\n",
    "    \"        'Systemic stress overlay',\\n\",\n",
    "    \"        'Supply limits from ESG',\\n\",\n",
    "    \"        'Retail-driven squeezes',\\n\",\n",
    "    \"        'Higher-resolution squeeze detector'\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"models_df = pd.DataFrame(models_data)\\n\",\n",
    "    \"print(\\\"EquiLend Model Catalog:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 80)\\n\",\n",
    "    \"display(models_df)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Load Sample Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"Load your EquiLend data or create sample data for testing.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create sample data for demonstration\\n\",\n",
    "    \"np.random.seed(42)\\n\",\n",
    "    \"n_days = 252  # One trading year\\n\",\n",
    "    \"n_stocks = 100\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generate date range\\n\",\n",
    "    \"dates = pd.date_range(start='2024-01-01', periods=n_days, freq='B')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create sample securities lending data\\n\",\n",
    "    \"sample_data = []\\n\",\n",
    "    \"for i in range(n_stocks):\\n\",\n",
    "    \"    ticker = f\\\"STOCK_{i:03d}\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Generate synthetic time series\\n\",\n",
    "    \"    base_fee = np.random.uniform(10, 500)  # Base borrow fee in bps\\n\",\n",
    "    \"    base_util = np.random.uniform(1, 95)   # Base utilization %\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for date in dates:\\n\",\n",
    "    \"        # Add some time series dynamics\\n\",\n",
    "    \"        fee_noise = np.random.normal(0, base_fee * 0.1)\\n\",\n",
    "    \"        util_noise = np.random.normal(0, base_util * 0.05)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        sample_data.append({\\n\",\n",
    "    \"            'Date': date,\\n\",\n",
    "    \"            'ticker': ticker,\\n\",\n",
    "    \"            'sec_desc': f'Sample Company {i}',\\n\",\n",
    "    \"            'industry': np.random.choice(['Technology', 'Healthcare', 'Finance', 'Energy', 'Consumer']),\\n\",\n",
    "    \"            'Fee All (BPS)': max(0, base_fee + fee_noise),\\n\",\n",
    "    \"            'Active Utilization (%)': np.clip(base_util + util_noise, 0, 100),\\n\",\n",
    "    \"            'On Loan Quantity': np.random.uniform(100000, 10000000),\\n\",\n",
    "    \"            'On Loan Quantity Month Diff': np.random.uniform(-50, 200),\\n\",\n",
    "    \"            'Fee All Month Diff (BPS)': np.random.uniform(-100, 100),\\n\",\n",
    "    \"            'Short Interest': np.random.uniform(1000000, 50000000),\\n\",\n",
    "    \"            'Average Daily Volume': np.random.uniform(500000, 5000000),\\n\",\n",
    "    \"            'Re-Rate Ratio': np.random.uniform(0.5, 3.0),\\n\",\n",
    "    \"            'B2B Loans': np.random.uniform(0, 1000000),\\n\",\n",
    "    \"            'Lender Count': np.random.randint(5, 50)\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"\\n\",\n",
    "    \"df = pd.DataFrame(sample_data)\\n\",\n",
    "    \"df['Date'] = pd.to_datetime(df['Date'])\\n\",\n",
    "    \"df = df.set_index(['Date', 'ticker']).sort_index()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Sample dataset created: {len(df)} rows, {len(df.columns)} columns\\\")\\n\",\n",
    "    \"print(f\\\"Date range: {df.index.get_level_values('Date').min()} to {df.index.get_level_values('Date').max()}\\\")\\n\",\n",
    "    \"print(f\\\"Unique tickers: {df.index.get_level_values('ticker').nunique()}\\\")\\n\",\n",
    "    \"display(df.head())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Core Factor Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"Apply all core short squeeze factors to the dataset.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Reset index for easier processing\\n\",\n",
    "    \"df_reset = df.reset_index()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Apply core factors\\n\",\n",
    "    \"print(\\\"Computing core factors...\\\")\\n\",\n",
    "    \"df_with_factors = compute_all_factors(df_reset)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show factor summary\\n\",\n",
    "    \"factor_cols = ['SIM', 'BCS', 'UPI', 'FTZ', 'DTC_z', 'LPF']\\n\",\n",
    "    \"factor_summary = df_with_factors[factor_cols].describe()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nCore Factor Summary:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"display(factor_summary)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot factor distributions\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 3, figsize=(15, 10))\\n\",\n",
    "    \"axes = axes.flatten()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, factor in enumerate(factor_cols):\\n\",\n",
    "    \"    df_with_factors[factor].hist(bins=50, ax=axes[i], alpha=0.7, color='steelblue')\\n\",\n",
    "    \"    axes[i].set_title(f'{factor} Distribution')\\n\",\n",
    "    \"    axes[i].set_xlabel('Z-Score')\\n\",\n",
    "    \"    axes[i].set_ylabel('Frequency')\\n\",\n",
    "    \"    axes[i].axvline(0, color='red', linestyle='--', alpha=0.7)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.suptitle('Core Factor Distributions', y=1.02, fontsize=16)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Extended Factor Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"Apply extended factors that use external data sources.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Apply extended factors\\n\",\n",
    "    \"print(\\\"Computing extended factors...\\\")\\n\",\n",
    "    \"df_extended = compute_extended_factors(df_with_factors)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show extended factor summary\\n\",\n",
    "    \"extended_cols = ['Borrow_CDS_Basis', 'Options_Skew_Div', 'ETF_Flow_Pressure', \\n\",\n",
    "    \"                'Macro_Stress', 'ESG_Constraint', 'Crowd_Buzz', 'SSR_v4']\\n\",\n",
    "    \"\\n\",\n",
    "    \"available_extended = [col for col in extended_cols if col in df_extended.columns]\\n\",\n",
    "    \"if available_extended:\\n\",\n",
    "    \"    extended_summary = df_extended[available_extended].describe()\\n\",\n",
    "    \"    print(\\\"\\\\nExtended Factor Summary:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 50)\\n\",\n",
    "    \"    display(extended_summary)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"\\\\n‚ö†Ô∏è  Extended factors require external data sources (FRED API, etc.)\\\")\\n\",\n",
    "    \"    print(\\\"Set up API keys to enable extended factor computation.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Factor Correlation Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Compute correlation matrix for all factors\\n\",\n",
    "    \"all_factor_cols = factor_cols + [col for col in extended_cols if col in df_extended.columns]\\n\",\n",
    "    \"correlation_matrix = df_extended[all_factor_cols].corr()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot correlation heatmap\\n\",\n",
    "    \"plt.figure(figsize=(12, 10))\\n\",\n",
    "    \"sns.heatmap(correlation_matrix, \\n\",\n",
    "    \"            annot=True, \\n\",\n",
    "    \"            cmap='RdBu_r', \\n\",\n",
    "    \"            center=0,\\n\",\n",
    "    \"            square=True,\\n\",\n",
    "    \"            fmt='.2f',\\n\",\n",
    "    \"            cbar_kws={'label': 'Correlation Coefficient'})\\n\",\n",
    "    \"plt.title('Factor Correlation Matrix', fontsize=16, pad=20)\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Find highly correlated factor pairs\\n\",\n",
    "    \"high_corr_pairs = []\\n\",\n",
    "    \"for i in range(len(correlation_matrix.columns)):\\n\",\n",
    "    \"    for j in range(i+1, len(correlation_matrix.columns)):\\n\",\n",
    "    \"        corr_value = correlation_matrix.iloc[i, j]\\n\",\n",
    "    \"        if abs(corr_value) > 0.7:  # High correlation threshold\\n\",\n",
    "    \"            high_corr_pairs.append((\\n\",\n",
    "    \"                correlation_matrix.columns[i],\\n\",\n",
    "    \"                correlation_matrix.columns[j],\\n\",\n",
    "    \"                corr_value\\n\",\n",
    "    \"            ))\\n\",\n",
    "    \"\\n\",\n",
    "    \"if high_corr_pairs:\\n\",\n",
    "    \"    print(\\\"\\\\nHighly Correlated Factor Pairs (|r| > 0.7):\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 50)\\n\",\n",
    "    \"    for factor1, factor2, corr in high_corr_pairs:\\n\",\n",
    "    \"        print(f\\\"{factor1} <-> {factor2}: {corr:.3f}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"\\\\nNo highly correlated factor pairs found.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Enhanced Short Squeeze Scoring\\n\",\n",
    "    \"\\n\",\n",
    "    \"Use the SSR v4 model to identify potential short squeeze candidates.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Get latest date data for screening\\n\",\n",
    "    \"latest_date = df_extended['Date'].max()\\n\",\n",
    "    \"latest_data = df_extended[df_extended['Date'] == latest_date].copy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Enhanced Short Squeeze Prediction\\n\",\n",
    "    \"ssr_v4 = EnhancedShortSqueezeV4()\\n\",\n",
    "    \"latest_data['SSR_v4_Score'] = ssr_v4.score(latest_data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Rank by squeeze potential\\n\",\n",
    "    \"top_squeeze_candidates = latest_data.nlargest(10, 'SSR_v4_Score')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nTop 10 Short Squeeze Candidates ({latest_date.strftime('%Y-%m-%d')}):\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 80)\\n\",\n",
    "    \"\\n\",\n",
    "    \"display_cols = ['ticker', 'sec_desc', 'industry', 'Fee All (BPS)', \\n\",\n",
    "    \"               'Active Utilization (%)', 'SSR_v4_Score']\\n\",\n",
    "    \"available_display_cols = [col for col in display_cols if col in top_squeeze_candidates.columns]\\n\",\n",
    "    \"\\n\",\n",
    "    \"display(top_squeeze_candidates[available_display_cols])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot squeeze score distribution\\n\",\n",
    "    \"plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"plt.subplot(1, 2, 1)\\n\",\n",
    "    \"latest_data['SSR_v4_Score'].hist(bins=30, alpha=0.7, color='orange', edgecolor='black')\\n\",\n",
    "    \"plt.title('SSR v4 Score Distribution')\\n\",\n",
    "    \"plt.xlabel('SSR v4 Score')\\n\",\n",
    "    \"plt.ylabel('Frequency')\\n\",\n",
    "    \"plt.axvline(latest_data['SSR_v4_Score'].quantile(0.9), color='red', linestyle='--', \\n\",\n",
    "    \"           label='90th Percentile')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.subplot(1, 2, 2)\\n\",\n",
    "    \"plt.scatter(latest_data['Fee All (BPS)'], latest_data['SSR_v4_Score'], \\n\",\n",
    "    \"           c=latest_data['Active Utilization (%)'], cmap='viridis', alpha=0.6)\\n\",\n",
    "    \"plt.colorbar(label='Utilization (%)')\\n\",\n",
    "    \"plt.xlabel('Fee All (BPS)')\\n\",\n",
    "    \"plt.ylabel('SSR v4 Score')\\n\",\n",
    "    \"plt.title('SSR Score vs Borrow Fee')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Daily Digest Generation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Generate automated daily digest using the consolidated data.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import daily digest generator\\n\",\n",
    "    \"sys.path.append('../src')\\n\",\n",
    "    \"from daily_digest import generate_digest\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate market summary stats\\n\",\n",
    "    \"avg_fee = latest_data['Fee All (BPS)'].mean()\\n\",\n",
    "    \"avg_fee_chg = (avg_fee / df_extended.groupby('Date')['Fee All (BPS)'].mean().iloc[-2] - 1) * 100\\n\",\n",
    "    \"avg_util = latest_data['Active Utilization (%)'].mean()\\n\",\n",
    "    \"avg_util_chg = (avg_util / df_extended.groupby('Date')['Active Utilization (%)'].mean().iloc[-2] - 1) * 100\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Market Summary for {latest_date.strftime('%Y-%m-%d')}:\\\")\\n\",\n",
    "    \"print(f\\\"Average Fee: {avg_fee:.2f} bps ({avg_fee_chg:+.2f}%)\\\")\\n\",\n",
    "    \"print(f\\\"Average Utilization: {avg_util:.2f}% ({avg_util_chg:+.2f}%)\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generate digest\\n\",\n",
    "    \"digest_result = generate_digest(\\n\",\n",
    "    \"    avg_fee=avg_fee,\\n\",\n",
    "    \"    avg_fee_chg=avg_fee_chg,\\n\",\n",
    "    \"    avg_util=avg_util,\\n\",\n",
    "    \"    avg_util_chg=avg_util_chg,\\n\",\n",
    "    \"    headline_1=\\\"MARKET DYNAMICS: Securities lending activity shows continued evolution\\\",\\n\",\n",
    "    \"    headline_2=\\\"FACTOR ANALYSIS: Multiple signals indicate shifting market conditions\\\",\\n\",\n",
    "    \"    headline_3=\\\"RISK MONITORING: Enhanced screening identifies key names to watch\\\"\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\n‚úÖ Daily digest generated: {digest_result['output_file']}\\\")\\n\",\n",
    "    \"print(f\\\"Data processed: {digest_result['data_rows']} rows\\\")\\n\",\n",
    "    \"print(f\\\"Key bullets: {len(digest_result['bullets'])}\\\")\\n\",\n",
    "    \"print(f\\\"Takeaways: {len(digest_result['takeaways'])}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display key insights\\n\",\n",
    "    \"print(\\\"\\\\nüìä Key Data Points:\\\")\\n\",\n",
    "    \"for i, bullet in enumerate(digest_result['bullets'], 1):\\n\",\n",
    "    \"    print(f\\\"{i}. {bullet}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüí° Key Takeaways:\\\")\\n\",\n",
    "    \"for i, takeaway in enumerate(digest_result['takeaways'], 1):\\n\",\n",
    "    \"    print(f\\\"{i}. {takeaway}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 10. Model Performance & Validation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Basic validation framework for factor performance.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Simple forward-looking validation\\n\",\n",
    "    \"# Note: This is simplified - real validation would use proper forward returns\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create synthetic forward returns for demonstration\\n\",\n",
    "    \"np.random.seed(123)\\n\",\n",
    "    \"df_extended['Forward_Return_5D'] = np.random.normal(0, 0.02, len(df_extended))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate Information Coefficients (IC) for each factor\\n\",\n",
    "    \"ic_results = {}\\n\",\n",
    "    \"for factor in all_factor_cols:\\n\",\n",
    "    \"    if factor in df_extended.columns:\\n\",\n",
    "    \"        # Group by date and calculate cross-sectional correlation\\n\",\n",
    "    \"        daily_ics = df_extended.groupby('Date').apply(\\n\",\n",
    "    \"            lambda x: x[factor].corr(x['Forward_Return_5D']) if len(x) > 10 else np.nan\\n\",\n",
    "    \"        ).dropna()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if len(daily_ics) > 0:\\n\",\n",
    "    \"            ic_results[factor] = {\\n\",\n",
    "    \"                'Mean_IC': daily_ics.mean(),\\n\",\n",
    "    \"                'IC_StdDev': daily_ics.std(),\\n\",\n",
    "    \"                'IC_IR': daily_ics.mean() / daily_ics.std() if daily_ics.std() > 0 else 0,\\n\",\n",
    "    \"                'Hit_Rate': (daily_ics > 0).mean()\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display IC results\\n\",\n",
    "    \"if ic_results:\\n\",\n",
    "    \"    ic_df = pd.DataFrame(ic_results).T\\n\",\n",
    "    \"    ic_df = ic_df.sort_values('IC_IR', ascending=False)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"\\\\nFactor Performance Summary:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 60)\\n\",\n",
    "    \"    print(\\\"Note: Using synthetic returns for demonstration\\\")\\n\",\n",
    "    \"    display(ic_df.round(4))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot IC time series for top factors\\n\",\n",
    "    \"    top_factors = ic_df.head(3).index.tolist()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    fig, axes = plt.subplots(len(top_factors), 1, figsize=(12, 8))\\n\",\n",
    "    \"    if len(top_factors) == 1:\\n\",\n",
    "    \"        axes = [axes]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for i, factor in enumerate(top_factors):\\n\",\n",
    "    \"        daily_ics = df_extended.groupby('Date').apply(\\n\",\n",
    "    \"            lambda x: x[factor].corr(x['Forward_Return_5D']) if len(x) > 10 else np.nan\\n\",\n",
    "    \"        ).dropna()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        daily_ics.plot(ax=axes[i], alpha=0.7, color='steelblue')\\n\",\n",
    "    \"        axes[i].axhline(0, color='red', linestyle='--', alpha=0.5)\\n\",\n",
    "    \"        axes[i].set_title(f'{factor} - Information Coefficient Over Time')\\n\",\n",
    "    \"        axes[i].set_ylabel('IC')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add rolling mean\\n\",\n",
    "    \"        daily_ics.rolling(20).mean().plot(ax=axes[i], color='orange', linewidth=2, label='20D MA')\\n\",\n",
    "    \"        axes[i].legend()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"‚ö†Ô∏è  No IC results available - insufficient data or factors\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 11. Export and Automation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Save results and prepare for automated execution.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save processed data\\n\",\n",
    "    \"output_dir = '../data'\\n\",\n",
    "    \"os.makedirs(output_dir, exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Export latest factor scores\\n\",\n",
    "    \"latest_scores = latest_data[['ticker', 'sec_desc', 'industry'] + all_factor_cols + ['SSR_v4_Score']]\\n\",\n",
    "    \"latest_scores.to_csv(f'{output_dir}/latest_factor_scores_{latest_date.strftime(\\\"%Y%m%d\\\")}.csv', index=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Export top squeeze candidates\\n\",\n",
    "    \"top_squeeze_candidates.to_csv(f'{output_dir}/top_squeeze_candidates_{latest_date.strftime(\\\"%Y%m%d\\\")}.csv', index=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Export historical IC data if available\\n\",\n",
    "    \"if ic_results:\\n\",\n",
    "    \"    ic_df.to_csv(f'{output_dir}/factor_performance_summary.csv')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"‚úÖ Data exported to {output_dir}/\\\")\\n\",\n",
    "    \"print(f\\\"   - Latest factor scores: {len(latest_scores)} securities\\\")\\n\",\n",
    "    \"print(f\\\"   - Top squeeze candidates: {len(top_squeeze_candidates)} securities\\\")\\n\",\n",
    "    \"if ic_results:\\n\",\n",
    "    \"    print(f\\\"   - Factor performance summary: {len(ic_df)} factors\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Automation suggestions\\n\",\n",
    "    \"print(\\\"\\\\nüöÄ Automation Suggestions:\\\")\\n\",\n",
    "    \"print(\\\"1. Schedule this notebook with Papermill for daily execution\\\")\\n\",\n",
    "    \"print(\\\"2. Set up data pipeline to refresh securities lending data\\\")\\n\",\n",
    "    \"print(\\\"3. Configure email alerts for high squeeze scores\\\")\\n\",\n",
    "    \"print(\\\"4. Implement real-time monitoring dashboard\\\")\\n\",\n",
    "    \"print(\\\"5. Add model performance tracking and alerting\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"This consolidated playbook provides:\\n\",\n",
    "    \"\\n\",\n",
    "    \"‚úÖ **Complete model implementation** - All core and extended factors in organized modules  \\n\",\n",
    "    \"‚úÖ **Automated data processing** - End-to-end pipeline from data to insights  \\n\",\n",
    "    \"‚úÖ **Performance validation** - IC analysis and factor correlation monitoring  \\n\",\n",
    "    \"‚úÖ **Daily digest generation** - Automated report creation with key insights  \\n\",\n",
    "    \"‚úÖ **Export capabilities** - CSV outputs for further analysis and monitoring  \\n\",\n",
    "    \"\\n\",\n",
    "    \"### Next Steps:\\n\",\n",
    "    \"1. Replace sample data with real EquiLend securities lending data\\n\",\n",
    "    \"2. Set up proper API connections for external data sources\\n\",\n",
    "    \"3. Implement production data pipeline and scheduling\\n\",\n",
    "    \"4. Add real-time monitoring and alerting capabilities\\n\",\n",
    "    \"5. Enhance validation with proper forward-looking returns data\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.5\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "03adb"
   },
   "source": [
    "## File Organization Philosophy\n",
    "\n",
    "This workspace uses a **hybrid approach** that leverages both notebooks and Python modules:\n",
    "\n",
    "### üìì **Notebooks for Interactive Work:**\n",
    "- **Analysis & Exploration** - This main playbook for daily analysis\n",
    "- **Model Development** - Individual notebooks for developing new factors\n",
    "- **Backtesting & Validation** - Interactive performance testing\n",
    "- **Visualization & Reporting** - Charts, reports, and presentations\n",
    "\n",
    "### üêç **Python Modules for Reusable Code:**\n",
    "- **Core factor classes** - So you can `from models.core_factors import ShortInterestMomentum`\n",
    "- **Utility functions** - Data loading, preprocessing helpers\n",
    "- **Production automation** - Scripts that run on schedule without human interaction\n",
    "\n",
    "### üîÑ **Workflow:**\n",
    "1. **Develop in notebooks** - Interactive experimentation and testing\n",
    "2. **Extract to modules** - Once code is stable and reusable\n",
    "3. **Import back to notebooks** - Clean analysis with modular components\n",
    "4. **Deploy modules** - For automated/scheduled processes\n",
    "\n",
    "This gives you the **best of both worlds**: notebook interactivity for analysis and clean modules for production code."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "vincent": {
   "sessionId": "f98c4db71d16e2d9e2c7e501_2025-06-29T20-03-56-307Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
