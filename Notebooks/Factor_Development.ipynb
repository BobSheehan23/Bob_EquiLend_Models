{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df08974",
   "metadata": {
    "cellUniqueIdByVincent": "88e34"
   },
   "source": [
    "# EquiLend Factor Development Notebook\n",
    "*Interactive development and testing of new factors*\n",
    "\n",
    "Use this notebook for:\n",
    "- 🧪 **Experimenting with new factor ideas**\n",
    "- 📊 **Testing factor performance on historical data**\n",
    "- 🔍 **Debugging and refining factor calculations**\n",
    "- 📈 **Visualizing factor behavior and distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10101dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EquiLend Core Factors\n",
    "# Consolidated short squeeze and securities lending factors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional, Union\n",
    "\n",
    "class ShortInterestMomentum:\n",
    "    \"\"\"Short Interest Momentum (SIM) - tracks accelerating short build-up\"\"\"\n",
    "    \n",
    "    def __init__(self, loan_col='On Loan Quantity Month Diff', fee_col='Fee All Month Diff (BPS)'):\n",
    "        self.loan_col = loan_col\n",
    "        self.fee_col = fee_col\n",
    "    \n",
    "    def score(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate SIM z-score\"\"\"\n",
    "        loan_momentum = df[self.loan_col].pct_change()\n",
    "        fee_momentum = df[self.fee_col].pct_change()\n",
    "        \n",
    "        combined = (loan_momentum + fee_momentum) / 2\n",
    "        return (combined - combined.mean()) / combined.std()\n",
    "\n",
    "class BorrowCostShock:\n",
    "    \"\"\"Borrow Cost Shock (BCS) - detects sudden fee spikes\"\"\"\n",
    "    \n",
    "    def __init__(self, fee_col='Fee All (BPS)', window=30):\n",
    "        self.fee_col = fee_col\n",
    "        self.window = window\n",
    "    \n",
    "    def score(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate BCS z-score based on rolling volatility\"\"\"\n",
    "        fees = df[self.fee_col]\n",
    "        daily_change = fees.diff()\n",
    "        rolling_std = daily_change.rolling(self.window).std()\n",
    "        \n",
    "        return daily_change / rolling_std\n",
    "\n",
    "class UtilizationPersistence:\n",
    "    \"\"\"Utilization Persistence (UPI) - persistent tight supply indicator\"\"\"\n",
    "    \n",
    "    def __init__(self, util_col='Active Utilization (%)', threshold=95, window=20):\n",
    "        self.util_col = util_col\n",
    "        self.threshold = threshold\n",
    "        self.window = window\n",
    "    \n",
    "    def score(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate UPI based on high utilization persistence\"\"\"\n",
    "        high_util = (df[self.util_col] >= self.threshold).astype(int)\n",
    "        persistence = high_util.rolling(self.window).mean()\n",
    "        \n",
    "        return (persistence - persistence.mean()) / persistence.std()\n",
    "\n",
    "class FeeTrendZScore:\n",
    "    \"\"\"Fee Trend Z-Score (FTZ) - detects under-the-radar fee drifts\"\"\"\n",
    "    \n",
    "    def __init__(self, fee_col='Fee All (BPS)', window=20):\n",
    "        self.fee_col = fee_col\n",
    "        self.window = window\n",
    "    \n",
    "    def score(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate FTZ based on fee slope trend\"\"\"\n",
    "        fees = df[self.fee_col]\n",
    "        \n",
    "        def rolling_slope(series):\n",
    "            x = np.arange(len(series))\n",
    "            return np.polyfit(x, series, 1)[0] if len(series) == self.window else np.nan\n",
    "        \n",
    "        slopes = fees.rolling(self.window).apply(rolling_slope, raw=True)\n",
    "        return (slopes - slopes.mean()) / slopes.std()\n",
    "\n",
    "class DaysToCoverZ:\n",
    "    \"\"\"Days-To-Cover Z-Score (DTC_z) - short covering pressure indicator\"\"\"\n",
    "    \n",
    "    def __init__(self, si_col='Short Interest', volume_col='Average Daily Volume'):\n",
    "        self.si_col = si_col\n",
    "        self.volume_col = volume_col\n",
    "    \n",
    "    def score(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate DTC z-score\"\"\"\n",
    "        dtc = df[self.si_col] / df[self.volume_col]\n",
    "        return (dtc - dtc.mean()) / dtc.std()\n",
    "\n",
    "class LocateProxyFactor:\n",
    "    \"\"\"Locate Proxy Factor (LPF) - proxy for locate availability\"\"\"\n",
    "    \n",
    "    def __init__(self, rerate_col='Re-Rate Ratio', b2b_col='B2B Loans'):\n",
    "        self.rerate_col = rerate_col\n",
    "        self.b2b_col = b2b_col\n",
    "    \n",
    "    def score(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate LPF based on re-rate ratio and B2B activity\"\"\"\n",
    "        rerate_z = (df[self.rerate_col] - df[self.rerate_col].mean()) / df[self.rerate_col].std()\n",
    "        b2b_z = (df[self.b2b_col] - df[self.b2b_col].mean()) / df[self.b2b_col].std()\n",
    "        \n",
    "        return (rerate_z + b2b_z) / 2\n",
    "\n",
    "# Helper function for z-score calculation\n",
    "def _z_score(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Calculate z-score for a pandas Series\"\"\"\n",
    "    return (series - series.mean()) / series.std()\n",
    "\n",
    "def compute_all_factors(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute all core factors for a dataframe\"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Initialize factor classes\n",
    "    sim = ShortInterestMomentum()\n",
    "    bcs = BorrowCostShock()\n",
    "    upi = UtilizationPersistence()\n",
    "    ftz = FeeTrendZScore()\n",
    "    dtc = DaysToCoverZ()\n",
    "    lpf = LocateProxyFactor()\n",
    "    \n",
    "    # Calculate factors (with error handling)\n",
    "    try:\n",
    "        result['SIM'] = sim.score(df)\n",
    "    except Exception:\n",
    "        result['SIM'] = np.nan\n",
    "        \n",
    "    try:\n",
    "        result['BCS'] = bcs.score(df)\n",
    "    except Exception:\n",
    "        result['BCS'] = np.nan\n",
    "        \n",
    "    try:\n",
    "        result['UPI'] = upi.score(df)\n",
    "    except Exception:\n",
    "        result['UPI'] = np.nan\n",
    "        \n",
    "    try:\n",
    "        result['FTZ'] = ftz.score(df)\n",
    "    except Exception:\n",
    "        result['FTZ'] = np.nan\n",
    "        \n",
    "    try:\n",
    "        result['DTC_z'] = dtc.score(df)\n",
    "    except Exception:\n",
    "        result['DTC_z'] = np.nan\n",
    "        \n",
    "    try:\n",
    "        result['LPF'] = lpf.score(df)\n",
    "    except Exception:\n",
    "        result['LPF'] = np.nan\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5adbb2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /\n",
      "Current sys.path: ['/Library/Frameworks/Python.framework/Versions/3.12/lib/python312.zip', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/lib-dynload']\n",
      "Added to sys.path: /MODULES\n",
      "core_factors.py exists: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Check current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Current sys.path:\", sys.path[:3])  # Show first 3 entries\n",
    "\n",
    "# Add the MODULES directory to the Python path\n",
    "modules_path = os.path.join(os.getcwd(), 'MODULES')\n",
    "if modules_path not in sys.path:\n",
    "    sys.path.insert(0, modules_path)\n",
    "    print(f\"Added to sys.path: {modules_path}\")\n",
    "\n",
    "# Verify the module file exists\n",
    "core_factors_path = os.path.join(modules_path, 'core_factors.py')\n",
    "print(f\"core_factors.py exists: {os.path.exists(core_factors_path)}\")\n",
    "\n",
    "# List contents of MODULES directory\n",
    "if os.path.exists('MODULES'):\n",
    "    print(\"MODULES directory contents:\", os.listdir('MODULES'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35771b17",
   "metadata": {
    "cellUniqueIdByVincent": "7e753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Factor classes loaded successfully\n",
      "Factor Development Environment - 2025-07-13\n",
      "Ready for interactive factor development!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "\n",
    "# Since the notebook is running from root, let's create the core_factors module inline\n",
    "# This is a workaround for the VFS path issue\n",
    "\n",
    "# Define the factor classes directly in this notebook\n",
    "class ShortInterestMomentum:\n",
    "    \"\"\"Short Interest Momentum (SIM) - tracks accelerating short build-up\"\"\"\n",
    "    \n",
    "    def __init__(self, loan_col='On Loan Quantity Month Diff', fee_col='Fee All Month Diff (BPS)'):\n",
    "        self.loan_col = loan_col\n",
    "        self.fee_col = fee_col\n",
    "    \n",
    "    def score(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate SIM z-score\"\"\"\n",
    "        loan_momentum = df[self.loan_col].pct_change()\n",
    "        fee_momentum = df[self.fee_col].pct_change()\n",
    "        \n",
    "        combined = (loan_momentum + fee_momentum) / 2\n",
    "        return (combined - combined.mean()) / combined.std()\n",
    "\n",
    "class BorrowCostShock:\n",
    "    \"\"\"Borrow Cost Shock (BCS) - detects sudden fee spikes\"\"\"\n",
    "    \n",
    "    def __init__(self, fee_col='Fee All (BPS)', window=30):\n",
    "        self.fee_col = fee_col\n",
    "        self.window = window\n",
    "    \n",
    "    def score(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate BCS z-score based on rolling volatility\"\"\"\n",
    "        fees = df[self.fee_col]\n",
    "        daily_change = fees.diff()\n",
    "        rolling_std = daily_change.rolling(self.window).std()\n",
    "        \n",
    "        return daily_change / rolling_std\n",
    "\n",
    "print(\"✅ Factor classes loaded successfully\")\n",
    "print(f\"Factor Development Environment - {date.today()}\")\n",
    "print(\"Ready for interactive factor development!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fe3893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d16d2519",
   "metadata": {
    "cellUniqueIdByVincent": "4353d"
   },
   "source": [
    "## 1. Load Test Data\n",
    "\n",
    "Start with sample or historical data for testing new factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec807d",
   "metadata": {
    "cellUniqueIdByVincent": "9b33a"
   },
   "outputs": [],
   "source": [
    "# Create sample data for testing\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2024-01-01', periods=100, freq='D')\n",
    "tickers = [f'STOCK_{i:02d}' for i in range(20)]\n",
    "\n",
    "# Generate sample securities lending data\n",
    "data = []\n",
    "for date in dates:\n",
    "    for ticker in tickers:\n",
    "        data.append({\n",
    "            'Date': date,\n",
    "            'ticker': ticker,\n",
    "            'Fee All (BPS)': np.random.uniform(10, 500),\n",
    "            'Active Utilization (%)': np.random.uniform(1, 95),\n",
    "            'On Loan Quantity': np.random.uniform(100000, 10000000),\n",
    "            'Short Interest': np.random.uniform(1000000, 50000000),\n",
    "            'Average Daily Volume': np.random.uniform(500000, 5000000)\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Test dataset: {len(df)} rows, {df['Date'].nunique()} dates, {df['ticker'].nunique()} tickers\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eec55f2",
   "metadata": {
    "cellUniqueIdByVincent": "15e81"
   },
   "source": [
    "## 2. Develop New Factor Ideas\n",
    "\n",
    "This section is for experimenting with new factor concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51840f",
   "metadata": {
    "cellUniqueIdByVincent": "ddd2f"
   },
   "outputs": [],
   "source": [
    "# Example: Develop a new \"Fee Volatility Spike\" factor\n",
    "class FeeVolatilitySpike:\n",
    "    \"\"\"Detect sudden spikes in fee volatility - experimental factor\"\"\"\n",
    "    \n",
    "    def __init__(self, fee_col='Fee All (BPS)', window=20, spike_threshold=2.0):\n",
    "        self.fee_col = fee_col\n",
    "        self.window = window\n",
    "        self.spike_threshold = spike_threshold\n",
    "    \n",
    "    def calculate(self, df):\n",
    "        \"\"\"Calculate fee volatility spike factor\"\"\"\n",
    "        # Group by ticker for time series calculations\n",
    "        results = []\n",
    "        \n",
    "        for ticker in df['ticker'].unique():\n",
    "            ticker_data = df[df['ticker'] == ticker].sort_values('Date')\n",
    "            \n",
    "            # Calculate rolling volatility\n",
    "            fee_returns = ticker_data[self.fee_col].pct_change()\n",
    "            rolling_vol = fee_returns.rolling(self.window).std()\n",
    "            \n",
    "            # Detect spikes: current vol vs average vol\n",
    "            avg_vol = rolling_vol.rolling(self.window * 2).mean()\n",
    "            vol_spike = rolling_vol / avg_vol\n",
    "            \n",
    "            # Create factor scores\n",
    "            ticker_data = ticker_data.copy()\n",
    "            ticker_data['Fee_Vol_Spike'] = vol_spike\n",
    "            ticker_data['Vol_Spike_Flag'] = (vol_spike > self.spike_threshold).astype(int)\n",
    "            \n",
    "            results.append(ticker_data)\n",
    "        \n",
    "        return pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Test the new factor\n",
    "fvs = FeeVolatilitySpike()\n",
    "df_with_new_factor = fvs.calculate(df)\n",
    "\n",
    "print(\"New Factor Statistics:\")\n",
    "print(df_with_new_factor[['Fee_Vol_Spike', 'Vol_Spike_Flag']].describe())\n",
    "\n",
    "# Visualize the new factor\n",
    "plt.figure(figsize=(12, 4))\n",
    "df_with_new_factor['Fee_Vol_Spike'].hist(bins=30, alpha=0.7)\n",
    "plt.title('Fee Volatility Spike Factor Distribution')\n",
    "plt.xlabel('Volatility Spike Ratio')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(2.0, color='red', linestyle='--', label='Spike Threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e4bbf",
   "metadata": {
    "cellUniqueIdByVincent": "6fdca"
   },
   "source": [
    "## 3. Compare with Existing Factors\n",
    "\n",
    "Test how the new factor performs relative to existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b101d7",
   "metadata": {
    "cellUniqueIdByVincent": "2f532"
   },
   "outputs": [],
   "source": [
    "# Compare with existing factors\n",
    "sim = ShortInterestMomentum()\n",
    "bcs = BorrowCostShock()\n",
    "\n",
    "# Add existing factors for comparison\n",
    "try:\n",
    "    df_comparison = df_with_new_factor.copy()\n",
    "    df_comparison['SIM'] = sim.score(df_comparison)\n",
    "    df_comparison['BCS'] = bcs.score(df_comparison)\n",
    "    \n",
    "    # Calculate correlations\n",
    "    factor_cols = ['Fee_Vol_Spike', 'SIM', 'BCS']\n",
    "    available_cols = [col for col in factor_cols if col in df_comparison.columns]\n",
    "    \n",
    "    if len(available_cols) > 1:\n",
    "        correlation_matrix = df_comparison[available_cols].corr()\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='RdBu_r', center=0, square=True)\n",
    "        plt.title('Factor Correlation: New vs Existing')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nFactor Correlations:\")\n",
    "        print(correlation_matrix)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not compute all factor comparisons: {e}\")\n",
    "    print(\"This is normal when developing new factors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67937e7d",
   "metadata": {
    "cellUniqueIdByVincent": "4d0df"
   },
   "source": [
    "## 4. Performance Testing\n",
    "\n",
    "Test factor performance with synthetic or historical returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593b7b7",
   "metadata": {
    "cellUniqueIdByVincent": "c9e6a"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic forward returns for testing\n",
    "np.random.seed(456)\n",
    "df_with_new_factor['Forward_Return_5D'] = np.random.normal(0, 0.02, len(df_with_new_factor))\n",
    "\n",
    "# Test predictive power\n",
    "def test_factor_performance(df, factor_col, return_col='Forward_Return_5D'):\n",
    "    \"\"\"Simple IC test for factor performance\"\"\"\n",
    "    valid_data = df[[factor_col, return_col]].dropna()\n",
    "    \n",
    "    if len(valid_data) < 20:\n",
    "        return {'IC': 0, 'Hit_Rate': 0.5, 'N': len(valid_data)}\n",
    "    \n",
    "    ic = valid_data[factor_col].corr(valid_data[return_col])\n",
    "    \n",
    "    # Quintile analysis\n",
    "    valid_data['Quintile'] = pd.qcut(valid_data[factor_col], 5, labels=False)\n",
    "    quintile_returns = valid_data.groupby('Quintile')[return_col].mean()\n",
    "    \n",
    "    # Hit rate: top quintile outperforms bottom quintile\n",
    "    hit_rate = (quintile_returns.iloc[-1] > quintile_returns.iloc[0])\n",
    "    \n",
    "    return {\n",
    "        'IC': ic,\n",
    "        'Hit_Rate': hit_rate,\n",
    "        'Top_Quintile_Return': quintile_returns.iloc[-1],\n",
    "        'Bottom_Quintile_Return': quintile_returns.iloc[0],\n",
    "        'N': len(valid_data)\n",
    "    }\n",
    "\n",
    "# Test the new factor\n",
    "performance = test_factor_performance(df_with_new_factor, 'Fee_Vol_Spike')\n",
    "\n",
    "print(\"New Factor Performance Test:\")\n",
    "print(f\"Information Coefficient: {performance['IC']:.4f}\")\n",
    "print(f\"Hit Rate (Top > Bottom): {performance['Hit_Rate']}\")\n",
    "print(f\"Top Quintile Return: {performance['Top_Quintile_Return']:.4f}\")\n",
    "print(f\"Bottom Quintile Return: {performance['Bottom_Quintile_Return']:.4f}\")\n",
    "print(f\"Sample Size: {performance['N']}\")\n",
    "\n",
    "print(\"\\n📝 Development Notes:\")\n",
    "print(\"- This is synthetic data for demonstration\")\n",
    "print(\"- Real testing requires historical price data\")\n",
    "print(\"- Consider regime analysis and stability testing\")\n",
    "print(\"- Factor may need refinement based on results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982d579",
   "metadata": {
    "cellUniqueIdByVincent": "254e3"
   },
   "source": [
    "## 5. Export Successful Factors\n",
    "\n",
    "Once a factor shows promise, export it to the models module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e127098b",
   "metadata": {
    "cellUniqueIdByVincent": "d878f"
   },
   "outputs": [],
   "source": [
    "# Example: Export new factor to models\n",
    "def export_factor_to_module(factor_class, factor_name, module_path='../models/experimental_factors.py'):\n",
    "    \"\"\"Helper to export successful factors to a module file\"\"\"\n",
    "    \n",
    "    import inspect\n",
    "    import os\n",
    "    \n",
    "    # Get the source code of the class\n",
    "    source_code = inspect.getsource(factor_class)\n",
    "    \n",
    "    # Create or append to experimental factors module\n",
    "    header = '''# Experimental Factors\n",
    "# New factors under development and testing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "'''\n",
    "    \n",
    "    if not os.path.exists(module_path):\n",
    "        with open(module_path, 'w') as f:\n",
    "            f.write(header)\n",
    "    \n",
    "    # Append the new factor\n",
    "    with open(module_path, 'a') as f:\n",
    "        f.write(f\"\\n\\n# {factor_name}\\n\")\n",
    "        f.write(source_code)\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"✅ Factor '{factor_name}' exported to {module_path}\")\n",
    "\n",
    "# Export our experimental factor\n",
    "# export_factor_to_module(FeeVolatilitySpike, \"Fee Volatility Spike Factor\")\n",
    "\n",
    "print(\"💡 Factor Development Complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Test with real historical data\")\n",
    "print(\"2. Validate on out-of-sample period\")\n",
    "print(\"3. Export to production models if successful\")\n",
    "print(\"4. Integrate into daily analysis workflow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vincent": {
   "sessionId": "c99783b7d7a23b8c1dd897a9_2025-06-29T20-12-55-246Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
